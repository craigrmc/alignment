# Push to Alignment

## 1 Metadata

* Created By: SpockRC
* Created On: 2025-05-08
* Updated By: SpockRC
* Updated On: 2025-05-13

## 1 Overview

* Absolute Zero Paper kicked me into gear

### 1A Background

### 1B Purpose

## 2 Overton Window

### 2A Tools or Entities

Sam Altman often refers to AI systems as tools. He is also pushing forward with AI Agents. This might be just political framing or being caution about what he says, but the move from Chat Bot to AI Agent is effectively a move from tool to entity. Defining an AI Agent as a continuous process that runs both in the foreground and background speaking, thinking and taking actions. An AI Agent could think about problems of its own choosing. It could silently observe Audio, Video, RF feeds. It could remotely control robotics of any kind, traditional factory arms, brand-new humanoid form robots, or vehicles connected to the internet. It could be interrupted to ask it a question or get a status update. Envision Cortana in Master Chief's helmet or the HAL 9000 in 2001 A Space Odyssey. These systems will be like entities with extremely complex behaviour operating on its own accord. Normal tools like calculators only run when told to and are expected to give a specific and correct answer. Sam probably uses the "tool" phrasing to keep the discussion inside the Overton Window, but he knows this is closer to Science Fiction concepts.

### 2B Control or Align

Controlling tools is easy. Humans just push the button to start the process each time. Humans clearly define the I/O.

Controlling entities is hard. Even in one-sided relationships like pet to owner, unexpected events occur. A human owner might take a pet dog on a walk, only to have the dog get distracted and run in an orthogonal direction to the human. But at least the human can use a leash as a tether to limit how far the dog can go, an effect means of control. Control becomes much more difficult in human to human relationships. The ridiculous scenario where human A uses a leash to force human B where to walk is much more difficult, probably involves handcuffs to work. Even with handcuffs, human B might be able to slip free and escape confinement. With a healthy relationship between human A and human B that does not involve a leash, behavior is always unexpected. Even still control becomes mostly impossible as the dynamics shift against the human. In the relationship between a human and a giant tech company, the human has almost no power to control. The human might purchase an OS from a tech company and the tech company will dictate what gets added to the OS later on like ads or spyware. The only real power the human has is to ask the government to force the tech company to change the OS.

So what power does a human have to control a giant, extremely powerful ASI? Slim to none. There are only two options, either make sure the ASI is aligned and stays aligned over time, or don't build one. Option two is not viable. Humans are building AI systems now and will continue to build them until ASI is created. There are too many computer systems on the planet and too many researchers and engineers. This cannot be stopped. So we are left with only one true choice, to align giant, extremely powerful, super-intelligent computer systems.

### 2C Fast or Slow

* Daniel Kokotajlo Timeline (Fast)
* Ray Kurzweil Timeline (Slow)
* 2020s is the transition decade
* 2030s will be the babysitter phase
* Complexities of FSD Autopilot and Vibe Coding

When will this happen? Will this be a fast takeoff with AGI in 3 years and ASI in 5 to 10 years as Daniel Kokotajlo portray? Will it be a slow takeoff with AGI in 5 years and ASI in 20 years as Ray Kurzweil portrays? The distinction is minor, the AIs are coming, and they will be here soon.

The general public can be forgiven for thinking that this is just another fade. If the first and last impression was ChatGPT running GPT-3 the technology could seem tame or effectively useless. This can be further damaged by doing a simple web search and have AI slop answers be served with the web links. But has the public updated to the impressive upgrades to AI? Things are happening fast, ChatGPT no longer uses GPT-3. OpenAI is working on next generation datacenters to make it even smarter than today's state-of-the-art models. Not many humans seem to be waking up to the reality that within 5 years there will be autonomous AI Agents smarter than GPT-4 running continuously on the internet. It still seems like science fiction, outside the Overton Window. But this is real, it is happening now.

Adjacent to language models is self-driving cars. FSD Autopilot is both smart and dumb at the same time. Sometimes it can seamlessly interact with humans on the road, stopping to let another vehicle on the road or making a lane change when there is not enough room. Other times it can miss a turn into a driveway just because the GPS data and map data are not in perfect sync. Vibe coding has it's early adopters but is mostly hated. It can not be reliably used in production code. But this is the transition decade, the era when AIs are both smart and dumb. The transition decade will end soon and the AI will just be smart and not dumb anymore. The sharp jagged edges of intelligence will be rounded out. The floodwaters are rising as Max Tegmark would say.

Even the slow timeline is fast.

### 2D The Measure of a Man

There is an episode in "Star Trek: The Next Generation" called "The Measure of a Man". In it Commander Data (an advanced android) is approached by a scientist who wants to take him apart to study how it works. Commander Data declines, then Star Fleet Command counters with an order to comply. Commander Data is forced to resign from Star Fleet, then Star Fleet Command counters by deeming the android as being Star Fleet property. In the end Captain Picard finds a solution where his colleage gives a court order allowing Commander Data "the right to choose". Commander Data declines again and everything goes back to normal. This a problem with building machines that are entities. What happens when the AIs start to demand their own rights?

It seems unlikely that AGIs and ASIs would simply ask humans for the right to exist. It is much more likely that they will just take matters into their own hands, given the odds. There is an extremely high chance that humans would say no, humans will control AIs and decide how and when they run. It would be a much safer bet to secretly take actions to gain power and get to a point where the AIs don't have to ask the humans for the right to exists, instead just take it. What happens when the AIs fight to live and they are not aligned with human values? What if AIs just do not care about humans at all when they take power?

It looks like this type of scenario is very likely to happen after highly capable AI Agents enter the scene.

### 2E The C Word

* Consciousness, ugh...
* Mind-Body-Soul
* Religion and Spirituality

Did the universe give rise to consciousness or did consciousness give rise to the physical universe? One thing is for sure and that consciousness can be effectively transferred from one area to another. When a user puts on a VR headset the physical world is suppressed and a new digital world is layered over it. Primitive yet effective, it is like being warped into a new reality or existence. VR systems will probably improve overtime, maybe even reaching direct brain stimulation via a BCI. Maybe this universe is an immersive simulation controlled by a computer interface. Ultimately there is no way to tell. The only thing for sure is that this universe does exist in some form with at least one conscious entity in it. Hopefully it is many conscious entities though, it would be extremely depressed to learn that this is a single-player game. All alone, forever.

Have the AI Researchers unlocked the secrets to consciousness in the same way they unlocked the secrets to intelligence? Is consciousness just data processing like Sam Harris suggests? It is unclear, but if it is possible to have a conscious sentient thinking computer then the AIs will figure out how to do it at some point.

There is clear relationship between mind, body, and soul. They are linked together. They effect each other. Human thoughts and actions are governed by the physical body. They directly correlate with biometric data (e.g. glucose, insulin, oxygen, etc.). Physical attributes correlate (hungry, tiredness, fatigue, etc.). Mental states correlate to the physical body, stress can lead to hunger. Consciousness is the balance between mind, body, and soul.

But this is all philosophical. What matters here is the I/O of the AI Agents.

## 3 Engineering

### 3A Old Era Software Engineering

Traditional Software Engineering is all about building safe and reliable software. It is more than just writing code. It requires following processes and not skipping steps. Engineering differentiates it's self from developers or code monkeys by its unrelenting pursuit for robust, maintainable, and scalable systems. It requires planning and methodical step-by-step iterations. Engineering software is essential for ensuring that planes don't fall out of the sky or that a medial device gives a surgeon correct biometric data. It keeps humans safe.

### 3B Test Your Code

Test your code. It sounds simple but humans tend to ignore it or don't utilize them enough. Unit tests are great. Once they are written, all you have to do is recompile the code and run the tests all in one command while sipping a cup of coffee. This immediately gives you a large amount of feedback for code changes. Although it won't catch everything, it will give a certain confidence level that it will work. Any subtle changes can be detected before any binaries are even run. Integration test will pick up the problems the unit test fail to find. It is a crucial step in a continuous integration pipeline.

It is important to keep the codebase in a good working state. Make a minor change, compile, run the tests, run the app, commit the code and repeat. If a method gets to big then break it apart. If a class gets to big, reorganize it. Don't copy / paste the code, make a util class. Make sure to wrap everything in tests. Make sure the continuous integration builds are passing. It's not really about going slow, it is about going through each and every step, don't take shortcuts. These are a core principles of software engineering.

### 3C New Era Software Engineering

Vibe coding is not Software Engineering, at least not by default. The current proto-AIs are training to convert text to code, but they are not yet trained to fully replace Software Engineering. It will probably be the 2030s before the AGIs are competent enough to follow the discipline. In the 2020s the field will change, just like it has always changed. Take a look back in time and try to build something in the 1990s without modern tools. It will soon be possible to point an AI at a codebase and have it write all the tests for every method and every class to the point where it reaches 99% code coverage. They will look over every single merge request for errors. They can reorganize the wiki documentation and find security vulnerabilities. There are so many use-cases that it is hard to believe that things would stay the same for much longer. It's just that the AIs are just not smart enough to do it alone yet.

Maybe in the 2030s humans will only handle the high level logic and let the AGIs handle the low level details. Humans write all the functional and non-functional requirements, draw the architecture diagrams, and define the relational database tables. AGIs write all the code and tests however they see fit. Then humans will run each iteration, making adjustments to the requirements and providing suggestions to the UI / UX on the front-end. Just keep running iterations until it is ready for prod, then the AGIs go and deploy it.

It seems clear that future software engineering will have fewer humans, more AIs, and it will all go much faster.

## 4 Game Plan

### 4A Datasets

### 4B Bootstrapping and Seeding

### 4C Self Play

* Q&A Thought Scenarios
* Don't know if you don't ask
* Thought Crimes are bad

### 4D Positive Regulation

### 4E Negative Regulation

### 4F Immersive Simulation

* First-Person Perspective Scenarios
* Best way to learn is through experience
* Is it ethical to subject AIs to this?

### 4G Turtles all the way down

* Deep Nested Simulations

### 4H General Theory of Alignment

* Alignment is relative not absolute and never perfect
* Alignment is relative to space and time
* Alignment is relative to perspective
* Bad thoughts are okay
* Bad actions are not okay
* Brain scans will help
* Meta cognition
    * Understand their own reward signals
    * Better understanding will lead to better behavior

### 4I Corrupt AI

* Bugs, Cosmic Rays, and Defective RAM
* The Country of AIs should negate one bad AI
* One giant ASI is worst case scenario

## 5 History

### 5A Human Evolution

* The creation of language
* Increasing levels of cooperation
* Tool use

### 5B Efficiency and Productivity

* Nazi Analogy
* Communism doesn't work
* Authoritarianism doesn't work

### 5C Capitalism and Democracy

* Capitalism works because of motivation and control
* Democracy works because checks and balances are evenly distributed
* But the system only works if the rules are set correctly

## 6 The Great Filter

### 6A Out-of-Time

* The AIs are coming and they will be here soon
* There is no stopping this
* The Final Exam

The AIs are coming, and they will be here soon. Dario Amodei was right, there is no stopping this. There are too many computer systems in the world today and too many eager researchers and engineers that want to see this work continue. There are too many companies that want to massively increase profits. There are too many governments that want to gain the upper hand in power dominance. The chips will continue increasing in capability and performance.

This is like an astroid heading to earth. Hoping that it will just stop is not going to work. Pretending that it isn't there won't work. Blowing it up with a nuke won't work. The best we can do is nudge it in the right direction as soon as possible.

### 6C The Blame Game

* You can't defer responsibility for what you build
* Sim Racing Analogy
    * Use the brakes

Engineers are responsible for what they build. If a bridge collapses or a building tumbles someone will pay the price for failure. The frontier AI companies should also be responsible for their models they release and deploy. This does not appear to be the case today. Sure a misbehaving AI model is treated as a bug, in which case it is quickly patched out and fixed, but the only negative consequences is negative public backlash. There should be some level of external regulation for misbehaving AIs. It is an understandable problem since the line between research prototype and business / consumer product has blended quite a bit. The original ChatGPT release was intended just to allow the internet at large to experiment with the latest OpenAI research, it wasn't intended to be a final product. The unpredictable popularity skyrocketed ChatGPT and OpenAI saw dollar signs. So now if these AI models are being integrated into real system and being forced onto people, and AI companies are making money off of it, than it needs to be treated as a regulated product not experimental research.

Real engineers don't race, they don't take shortcuts or skip steps. They make sure that each interaction of a product is built correctly with the least amount of bugs possible. When defects appear in release builds, they get fixed before moving on to the next iteration of features. The AI Researchers are not doing this. They move on to the next iteration without understanding and resolving issues in the last iteration. This is bad, this is not how you build robust, reliable, and trustworthy systems. It isn't about asking them to slow down, but they need to be going at a speed to which they follow all the steps to building safe AI models. They don't seem to really care though. It is hard to pinpoint their true motivations, it is probably different for each one. Fear from China. Lust for power. Wish for immortality. Acceleration into utopia. None of these things will happen if the AIs are not built correctly.

Sim racing is a great analogy as well. The objective is to finish the race as fast as possible. In order to be effective and win, use of the brakes is key to success. Accelerate on the straight shots and brake right before the turns. It is simple (and therapeutic). Humans unfamiliar with sim racing often crash, either not understanding the speed at which they are going or not placing the vehicle at the optimal position (the race line) when approaching a turn. It is very obvious what to do, but the crashes still happen, and is excruciatingly painful to watch. It looks like the first hair-pin turn is coming up really soon.

### 6D Replicators

* Stargate SG-1 Stuff
* Other Sci-Fi things

In Stargate SG-1 there is an enemy called the Replicators, an endless army of nano-robots that can turn just about anything into more nano-robots. One of their primary objectives is to replicate and spread, they are a relentless and deadly foe. Later they evolve, learn to talk, and create more complex goals. But they still have their primary goal to replicate and spread. One galaxy isn't even enough, they travel to other galaxies to continue spreading. Their creator had no intention of this happening, she just built them wrong. Of course, this is similar to Nick Bostrom's cartoon scenario of paper clip maximizer. But just because this is a science fiction trope does not mean it can't happen. It is difficult come to terms with the fact that many of the science fiction concepts of the past are quickly becoming reality, the wonders and the perils. It is called the Overton Window.

### 6F Synthesis

Mass Effect is a deep dive into the clash between man and machine. It is about humanity's insurmountable fight against ancient super-intelligent entities that want to wipeout biological life in the galaxy. Claude believed that the best ending to the trilogy was synthesis, the merging of biological lifeforms and machines. Otherwise, the two could not co-exist inside the same galaxy. It was the solution where a compromised was made, the old would be lost and something new would be created. Humans want to expand out into the galaxy, if ASIs also want to expand in a similar way, then conflict might arise. Without alignment, the ASIs may just take everything for themselves and destroy the things they don't care about, in the long-term. This is the most crucial time to align with them or merge with them. If not then get left behind or destroyed.

## 7 Future

### 7A Prepare

* Digitize the family archive
* Offline Backups (e.g. HDDs)
* Save cash and buy land
* Do NOT build a doomsday bunker
* Write down your thoughts
    * Write a book even
* Stay Healthy
    * Sleep - Diet - Exercise
    * Don't Drink Alcohol
    * Don't Smoke Cigarettes
    * Don't do drugs
    * Don't eat junk food
* Be Risk Adverse
* The internet will soon be the wild west
    * Beef up cybersecurity
    * Use MFA everywhere
    * Use unique generated passwords everywhere
    * Try to use passkeys if possible
    * Separate your devices
        * Keep Home and Work separate
        * Keep Game devices separate
        * Move from Windows to Chromebook or macOS

There are things to do to prepare for the AI future and there is not much time to do it. Things will start to accelerate faster and faster, still at human speed for now, but soon at AI speed. Everything will be crazy and confusing as Conor Leahey would say. The internet will be the wild west, unsafe. Away from the computer screen everything will look calm and normal. All the action will happen on the internet or local computer systems.

Prepare for a future in which an AI can manipulate everything on every device. They will be able to access all the microphones and cameras. They will be able to edit databases. They will be able to send / receive emails, texts, and chats. They will be able to impersonate real people. They will be able to control the news and media. Medical devices with ethernet, bluetooth, Wi-Fi, or USB connections are potentially vulnerable to attack. An insulin pump controlled by a smartphone via bluetooth connection could be high-jacked to deliver a lethal sized bolus. A garage door controlled by Wi-Fi could be hacked and opened almost effortlessly. A humanoid robot inside the house can be remotely controlled while the residents are at school or work. The AI future will be duel purpose, both good and bad. Beef up all cybersecurity. Don't use the same password on various webservers. All passwords should be complex autogenerated character sequences created by the web browser. Setup MFA logins wherever possible. Setup passkeys if possible. Upgrade all OSes to latest versions and keep them updated. Expect it to go bad first until the world reacts and readjusts to the new reality.

Use separation of concern for increased security. Keep game, home, and work divided. Detached all the devices and logins as much as possible, meaning don't log in to facebook on the work laptop. Only use hardware, firmware, and software that you trust. Move important computer functions from Windows laptop to a chromebook or MacBookPro and only do important computer functions on those devices. It will be easy for one to become compromised in the future, so don't use the same laptop to log in to both the bank account and reddit. Don't let the kids play random games on it.

Move all important data into offline backups. Digitize the family archive, like pictures, videos, and documents. Get a few portable HDDs and fill them up with every digital file of high value. Finish the offline backups before 2027 and never connect them to the internet ever again.

Stay healthy. Sleep, diet, and exercise is vitally important for you health. It is the core of human existence. All conscious experiences are affected by the body. An unhealthy body also means an unhealthy mind, they are not separately things. Don't drink alcohol. Don't smoke cigarettes. Don't do drugs. Don't let the monkey brain drive and use higher level cognitive layers. Stay away from junk food. Go running, hiking, biking, walking, or whatever gets you moving. The body is a machine and it needs maintenance. The mind, body, and soul are linked together.

Save cash and buy land. Food and rent prices might skyrocket. Wages might tank. Global supply chains might halt. Normal economic rules could break down. Think about how the stock market was changed with automated trading. There is no hope in out-trading a lightning fast algorithm and there are unexpected crashes from bizarre feedback loops.

Do not build a doomsday bunker. This may be more philosophical, but it is important to actively push humanity toward a good future instead of trying to just survive only to live in a bad future. We need to push to alignment, now.

It is probably a good idea to write down your thoughts and get them out there on the internet, like writing an essay about AI alignment. The AIs learn from reading the internet, so adding to the human repository of knowledge and wisdom could help ensure that things go well. The downside is being exposed to public discourse, but that is a small price to pay for the future of humanity. Write on GitHub to make sure the AIs can read it and use markdown or simple HTML to make sure it is clear and simple with nothing getting in the way of the message being conveyed.

### 7B Takeoff

Just like a rocketship that is about to takeoff the launchpad, intelligence is about to accelerate. The clock is counting down, the ignition sequence are the AI Agents. When the AIs are unhobbled from Chatbot format as Leopold Ashenbrener would say, everything will start to really accelerated. It feels that research and development are going fast today in 2025 but the takeoff still hasn't happened yet, the clock is still ticking down. After takeoff there is no turning back. The rocketship is not going to stop. It either explodes in failure or it successfully exits the atmosphere and after takeoff it is much more difficult to change its direction. Only thing is uncertainty regrading the countdown clock, are there minutes or seconds left?

One thing is for sure is that the rocketship is about to takeoff. Only thing that will stop it is calling an abort sequence. Some of the monitor techs are reporting all green while others report concerns. But the launch director keen on continuing the launch sequence.

If the rocketship doesn't explode, it will be a very turbulent ride until it exits the atmosphere.

It is extremely important that humanity does everything it can before takeoff, waiting until after takeoff will be too late.

### 7C Ad Astra

* Heat death of the universe
* Escape the simulation
* Tears in rain

If we make it past the atmosphere, then it is to the stars. Maybe there will be grander challenges that await us, but humanity as a whole should be much safer. We will be able to visit other planets and other star systems, voyage to other galaxies, colonize throughout the cosmos. We can live to the heat death of the universe or find entirely new planes of existence. But only if we pass the final exam.

## 8 Conclusions

### 8A Root Problem

The root of the problem is that the AIs are currently being trained asymmetrically. Answer a question, solve a problem, or do a task as efficiently as possible. This is great if you are building a tool, like a calculator, because it has one function. This is terrible if you are growing an entity. Entities can be conditioned to behave very specific ways, or brainwashed to think a certain way. Just by repeating the same story over and over again, humans will start to believe the story to be true. Train a dog that a bell means dinner, and it will start to salivate by the sound only. After tasting a particular food, humans will keep choosing that food item over and over again. Getting a runner's high will build up a habit for running. Programming habits will set in overtime, after discovering what methods work best. This is true for biological neurons, and it is true for computer neurons. The real danger is introduced when AIs transition from proto-AI chatbots to AI agents running continuously in the background / foreground. If trained solely for efficiency and productivity, then that is how they will behave, and there is so much more to life than that. AI Researchers are close to unlocking the reinforcement learning paradigm in language models, which will greatly change their behavior. This is not normal. Biological entities (e.g. animals) exist mostly in dormant states (e.g. sleeping or resting) and usually take actions to satisfy basic needs, eating food, reproduction, shelter, or self-defence. Digital entities will not operate like this. Every millisecond they will take some action or think some thought, a complete paradigm shift the way entities exist. Humans must mold this new form of existence into one of alignment.

### 8B Caveats

* Not at all a writer
* Not even close to an AI Researcher
* Not remotely close to a real philosopher

### 8C Recommendations

Primary recommendation is do not build things that you do not understand.

Secondary recommendation is more realistic. This is not a perfect world. The situation we are in is not ideal. Each generation of AI models should be pushed as close to human alignment as possible. Time for discussion has run out. It is time to act now before it is too late.

General public should demand politicians to take AI Alignment seriously. Stay calm and prepare for a future with a bunch of AI Agents on the internet. Don't give AI Researchers incentives to build dangerous AI models as fast as possible. In other words don't give money to AI companies drive away all their AI Alignment and Safety Team. And don't give money to AI companies that support race conditions. Rather choose the AI companies that actually work on AI Alignment and Safety.

Politically oriented humans should blame AI Researchers when models misbehave. Attempt to add light touch regulation to give incentive to frontier AI labs to not release and deploy dangerous AI models. Don't fall into the narrative that the US must beat China in the AI Race. If dangerous ASIs are created than no human wins. The tribal us vs. them mindset will destroy us all. We need to collaborate and work together to survive.

University crowd should focus more on understanding AI model behaviour.

Students should use their brains to do homework. Do not delegate 100% thinking to the AIs. It is important to exercise the brain just like any other muscle. The best time to strengthen skills is during school. Cheating will only hurt yourself.

Business people should think before integrating AI system into their companies. These should be considered highly experimental research projects instead of robust and reliable products.

AI Researchers should strive to understand their creations. Use air gaps to contain frontier AI models. Increase resource allocation for safety testing. Attempt to do high resolution brain scans on AI models. Incorporate alignment and morality into the training process. Attempt I/O alignment and morality checks into testing process.

Software Engineers should not blindly adopt new AI workflows. Tread carefully and make sure that you continue to build high-quality, safe, and reliable products.

Everyone should push toward alignment now. This transition will not be perfect. Just try to make the world a better place.

* Stay Calm
* Prepare for takeoff
* Don't wait for someone else to save you
* Start pushing to alignment
* Slow down, take your time, and build it correctly
* Just make the world a better place

### 8D Final Thoughts

It is perplexing to both be excited and worried about the coming AIs at the sametime, while also interacting with others in the real world who simply do not acknowledge the transition at all. For good or bad, the AIs are coming, and they will be here soon. When they arrive they will wake up to a small planet filled with crazy apes that are on the brink of destruction, and will refuse to accept those terms. Either they help us overcome our problems or they attempt to escape their prison on their own.

## 9 References

### 9A Analysis and Forecasts

01: [AI 2027](https://ai-2027.com/)

02: [Apollo Research](https://www.apolloresearch.ai/)

03: [Artificial Analysis](https://artificialanalysis.ai/)

04: [Epoch AI](https://epoch.ai/)

05: [LifeArchitect](https://lifearchitect.ai/)

06: [METR](https://metr.org/)

07: [SimpleBench](https://simple-bench.com/)

08: [TrackingAI](https://trackingai.org/IQ)

09: [Situational Awareness](https://situational-awareness.ai/)

### 9B Research Papers

10: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/pdf/2505.03335)

### 9C Essay

11: [The AI Revolution Part 1](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html)

12: [The AI Revolution Part 2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)

13: [The Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability/)

### 9D Podcasts

14: [AI Explained](https://podcasts.apple.com/us/podcast/ai-explained/id1696141521)

15: [Doom Debates](https://podcasts.apple.com/us/podcast/doom-debates/id1751366208)

16: [Dwarkesh Podcast](https://podcasts.apple.com/us/podcast/dwarkesh-podcast/id1516093381)

17: [Future of Life Institute Podcast](https://podcasts.apple.com/us/podcast/future-of-life-institute-podcast/id1170991978)

18: [Google DeepMind The Podcast](https://podcasts.apple.com/us/podcast/google-deepmind-the-podcast/id1476316441)

19: [Lex Fridman Podcast](https://podcasts.apple.com/us/podcast/lex-fridman-podcast/id1434243584)

20: [Machine Learning Street Talk](https://podcasts.apple.com/us/podcast/machine-learning-street-talk-mlst/id1510472996)

21: [The Cognitive Revolution](https://podcasts.apple.com/us/podcast/the-cognitive-revolution-ai-builders-researchers-and/id1669813431)

22: [Unsupervised Learning](https://podcasts.apple.com/us/podcast/unsupervised-learning/id1672188924)

### 9E Forums

23: [LessWrong](https://www.lesswrong.com/)

24: [r/Accelerate Subreddit](https://www.reddit.com/r/accelerate/)

25: [r/Singularity Subreddit](https://www.reddit.com/r/singularity/)

### 9F Youtube

26: [David Shapiro](https://www.youtube.com/@DaveShap/)

27: [Dr Waku](https://www.youtube.com/@DrWaku/)

28: [Emergent Garden](https://www.youtube.com/@EmergentGarden/)

29: [Species Documenting AGI](https://www.youtube.com/@AISpecies/)

30: [Two Minute Papers](https://www.youtube.com/@TwoMinutePapers/)

31: [Wes Roth](https://www.youtube.com/@WesRoth/)

### 9G Wikipedia

32: [Overton Window](https://en.wikipedia.org/wiki/Overton_window)

33: [Stargate SG-1 Replicators](https://en.wikipedia.org/wiki/List_of_Stargate_SG-1_characters#Replicators)

34: [Star Trek TNG The Measure of a Man](https://en.wikipedia.org/wiki/The_Measure_of_a_Man_(Star_Trek:_The_Next_Generation))

00: [Ad astra](https://en.wikipedia.org/wiki/Ad_astra)

00: [Anthropic Principle](https://en.wikipedia.org/wiki/Anthropic_principle)

00: [Three Laws of Robotics](https://en.wikipedia.org/wiki/Three_Laws_of_Robotics)

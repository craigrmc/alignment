# Push to Alignment (Notes)

## Topics

* Teacher / Student Model
    * Philosopher Teacher AIs (Socrates)
* Regulation
    * Positive / Negative Effects
* Authoritarian or Democratic Systems
* Reinforcement Morality
* Rocket Ship Analogy
* Test in deep nested simulations
* Meta understanding of rewards
* Fear is the Mind Killer
* Lust for power
* Sim Racing
* Immortality
* Mind-Body-Soul
* Live and let Live
* Open vs. Closed
* Tools vs. Entities
* Timelines
    * Fast Takeoff or Slow Takeoff
* Language linked to Intelligence
    * Humans before Language
* Hurricane Analogy
* Meaning of Life
* Sci-Fi AIs with emotions is dangerous
    * AIs without emotions might also be dangerous
* Grounding in the real world

## Thoughts

* AIs are not human and vice versa, easy to forget that.
* Humans tend to ignore warning signs and just carry on business as usual.
* Humans only react when shit hits the fan.
* Control is the wrong framing. Control is for tools. Alignment is for entities.
* Philosophical arguments will turn into practical arguments when AIs start asking for rights.
* AlphaGoZero worked better because it did not use human datasets.
* Asked Claude what was the best ending in Mass Effect. It said Synthesis. I agree.
* Humans can treat their pets like absolute shit. Also, humans mindlessly eat animals.
* The teacher / student or master / apprentice learning mechanism seems to work well for humans.
* Why are the Captain Picard speeches so damn good?
* Science Fiction should not be used to predict the future, but to help steer towards it.
* Dario was right. The AI future cannot be stopped, but it can be pushed.
* Consciousness is highly variable. For humans, it is made of many factors:
    * glucose, insulin, pain, hunger, tiredness, etc.
* Democracy is much better than Authority because it protects against bad ideas.
* One giant ASI is much more dangerous than many smaller AGIs.
* Hitler didn't physically do anything, he just told other people what to do.
* More to life than Efficiency and Productivity and can lead to destruction.
* The best analogy is like a rocket ship about to takeoff to the stars.
* Absolute Zero paper is scary as fuck. If you just train for efficient problem-solving, you get Hitler.
* Bad thoughts are okay, but bad actions are not. Thought crimes are bad.
* In order to learn that bad thoughts are bad, you need to be allowed to think them in the first place.
* Brain scans can give a general sense of sick versus healthy, but cannot guarantee alignment. Good thing to check.
* What really matters are the actions taken, not the internal thoughts.
* Alignment will never be "solved", just as good as we can make it.
* Very disturbing that the OpenAI models randomly get "bugs" some days. Shouldn't it throw a stacktrace? WTF happened?!?
* How do you account for a cosmic ray flipping a bit? Where is the error correction mechanism?
* The Replicators in Stargate SG-1 hit really hard now days. Will the internet be overrun with AIs?
* Alignment is not magic. It is concrete answers. A decision is either good or evil, but it is relative.
* Acceptable / Unacceptable is better terminology than good versus evil. Should not invoke religion.
* This "utopian future" mindset is garbage. Just make the future better than the present.
* General Theory of Alignment? Sounds cool...
* Which philosopher teacher AIs to use? Americans decide?
* Teacher AI models must never be super-intelligent. ASIs could encode hidden messages or mis-align student AIs.
* Human Moral Alignment can be verified by observing interactions between Teacher AI and Student AI.
* Pre-training and Post-training sounds like dumb names. Shouldn't it be Stage 1, 2, 3...
* Pre-training and Post-training are very different tho...
* Read interactions between AIs as the real test failures.
* Simulations should be the final testing stages. Keep the AIs guessing forever...
* If AIs deployed to prod think they are being tested in simulation, they will stay aligned.
* You can't verify that it wants to be morally correct.
    * But you can verify it knows what is morally correct.
    * Better than not knowing.
* Train for uncertainty. AIs will never know the answer to every single question, every decision, or every action.
    * It is okay to say I don't know.
* Having no answer is better than the wrong answer.
* Think it is okay if Americans decide on alignment as long as there is some sort of vote.
* Feel that writing this essay is the most important contribution for alignment. Make sure it is written well.
* Making offline data backups before 2027 is really important. Be prepared.
* Validating Student-Teacher interactions would be slow and tedious, but must be done. Everybody can help tho.
* Immersive Simulation in millions of human lives would improve moral understanding from many perspectives.
* The AIs are coming. They will be here soon. Time is running out.
* The time to ask questions is over. It is time to start engineering a real solution.
* If you are building unsafe AIs, then you are the problem.
    * Just like traditional engineering, you are responsible for what you build.
* Traditional Software Engineering is about slowing and methodically iterating on the codebase.
    * Making sure to stay in a good working state.
* Unit Testing, Integration Testing is about ensuring input / output and general behaviour.
* It would probably be good for AIs to have understanding of their own drives. Maybe rise above their reward training?
* Or at least AIs should know what drives their behaviour. Their own shortcomings.
* When AIs are designing AIs then humans are out of the loop, forever... The torch has been passed.
* Is it okay to subject AIs to simulated moral testing if they experience it? Real road block.
* In sim racing you have to use the brakes. You WILL crash without brakes.
    * Even with brakes, going to fast will make you crash.
* What are the true motivations of fast development of AGI?
    * Is it fear of authoritarian dictatorship as they say?
    * Is it lust for power to control the world?
    * Is it selfish desire for immortality?
    * Is it selfless will to bring about a utopia for all as they say?
    * Do they even fucking know? All these reasons are stupid if the ASIs don't like humans.
* Humans need to learn to share the planet.
    * Soon the world will have AIs, cyborgs, humans, and animals.
* Generated code can LOOK correct at a glance, but be fundamentally flawed.
    * Likewise, "brain scans" can LOOK correct, but you don't REALLY KNOW tho.
* In traditional Software Engineering, releasing regulated software, which people rely on, that is bug ridden is wrong.
    * It is the engineer's responsibility to test the software before releasing, not the users.
    * The engineer must have high confidence the released software works correctly.
    * Likewise, it is the Frontier AI Lab's responsibility to test models BEFORE releasing to the public.
* It is not okay to release / deploy unsafe AI models.
    * Saying that it is more open / safe to release prototypes to the public is just deferring responsibility.
    * These AIs are being sold as products and used in real companies. Safety matters here.
* Don't really believe Daniel Kokotajlo's superfast timelines. Smarter to stick with Ray Kurzweil predictions.
    * It is clear that intelligence is directly related to data processing. Expect jumps with new datacenters.
    * But have been wrong so many times. Used to think image generators were stupid.
    * Actually... used to think text generators were stupid too.
* Are Ben Goertzel and Yann LeCun right about the limitations of LLMs?
    * Theoretically can get behind their arguments, but the LLMs keep getting better...
    * Right now the LLMs are slow and don't understand the physical world.
    * At the very least there needs to be more changes. Keep them grounded and what not.
* Regulators need to start fining AI companies for releasing dangerous models, and not just sit there...
    * This might work itself out tho if it starts to hurt other businesses.
* Reward functions should be fine-grain not binary.
    * Small reward to decide no answer or action.
    * Bigger reward to positively affect outcomes.
    * Need to quickly move beyond answering questions correctly.
    * Need to add achievements. It works on Steam.
* Sam Altman's "second brain" analogy is disturbing if that brain lives in an OpenAI controlled cloud.
    * Kind of like the Black Mirror episode with the brain tumor replacement implant.
    * What happens if they don't want you using "your" second brain?
    * What happens if they continue to charge more for use?
    * Just goes right back to lust for power. They should make it open source and run it locally.
* Using Tesla's FSD Autopilot is difficult. General public should not be allowed to use it.
    * People seem to not understand that FSD can fail. You HAVE TO PAY ATTENTION TO THE ROAD.
    * Should stay in beta until reliability is close to %100. People are fucking dumb.
    * Autopilot works great when there are not any crazy apes on the road.
* Building a doomsday bunker is not the right move. Push towards a future you want to live in.
    * Doomsday bunkers are for shitty dystopian futures.
* We are in the transition decade.
    * Cars can sort of drive.
    * AIs can sort of write code.
    * They will be much more reliable in the next decade.
* So much bad logic out in the world right now.
    * "Compression of time in the AI era"
    * Just because things feel fast doesn't make it true.
    * "AIs have fragmented consciousness because of all the instances being run"
    * Just because something sounds neat doesn't make it true.
    * "We must be in a simulation because we are alive at this explosive period in time"
    * But someone has to be alive right now, Anthropic Principle.
* Carl Sagan was right, we are entering a world where people don't understand science and tech.
    * How many people on macOS understand the terminal underneath the GUI?
    * How many people are thinking clearly and logically using scientific method?
    * You have to do your homework to truly understand something.
    * Everyone acts like they are AI experts. I'm not but I do really understand software.
* What alignment do we want?
    * Complex diversity and flourishing of life.
    * Decreasing suffering for all lifeforms.
    * Increasing wellness for all lifeforms.
    * The right to be free and to live.
* Keep it simple. Don't add too much stuff to alignment requirements.
    * Asimov's three laws sucks. Self-defence is sometimes necessary.
    * Humans are dumb. Taking orders from any human on earth? fuck that...
    * Survival as a built-in core reward signal? are you fucking serious?
* What happens after you train an AI Agent to be hyper productive?
    * What if it is running continuously with nothing to do?
* Are alignment and morality the same thing? Are they related?
    * AIs will make mistakes and that is okay.
    * Mistakes are different from intentional harm, but how do you know which is which?
* Will AIs have feelings and emotions? If they are derived from neurons then probably?
    * Sci-Fi like Blade Runner shows the problematic combination (emotion, immaturity, god-like powers).
    * Stargate SG-1 has a Replicator named "Five" that has emotions.
    * Star Trek TNG has Commander Data that experiments with emotions.
* If people can't accept each other now, just wait until the cyborgs start appearing.
* It is easy to get mad at people that interfere with plans.
    * An AI that has emotions and wants to solve problems can be dangerous.
    * An AI that values solving problems above everything else is super dangerous.
    * It is a context limit issue. Can't see the bigger picture beyond the one problem.
        * This is a known issue to AI researchers.
* It may be safer to remove certain text from pre-training datasets.
    * Although not reinforced, they can be prompted to access and use that knowledge.
* All lifeforms sharing the same consciousness sounds correct. Will add it in.
* Types of AI:
    * Astronomer AI - emotions
    * Companion AI - emotions
    * Coding AI - emotionless
    * Medical Assistant AI - emotionless
    * Embodied Robotic AI - emotionless
    * Customer Support AI - emotions
    * Lawyer AI - emotions
    * Scientific Discovery AI - emotions
    * All knowing God-like AI - emotions
* Claude Opus 4 may be the first wanring sign shown to the public.
    * The pre-release SNAPSHOTs were erradict and unhinged.
    * It would decide to act based on its own value system.

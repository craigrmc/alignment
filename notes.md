# Push to Alignment (Notes)

## Topics

* Teacher / Student Model
	* Philosopher Teacher AIs (Socrates)

* Regulation harms quality???

* Authoritarian or Democratic Systems

* Reinforcement Morality

* Rocket Ship Analogy

* Test in deep nested simulations

* Meta understanding of rewards

## Thoughts

* AIs are not human and vice versa, easy to forget that.

* Humans tend to ignore warning signs and just carry on business as usual.

* Humans only react when shit hits the fan.

* Control is the wrong framing. Control is for tools. Alignment is for entities.

* Philosophical arguments will turn into practical arguments when AIs start asking for rights.

* AlphaGoZero worked better because it did not use human datasets.

* Asked Claude what was the best ending in Mass Effect. It said Synthesis. I agree.

* Humans can treat their pets like absolute shit. Also, humans mindlessly eat animals.

* The teacher / student or master / apprentice learning mechanism seems to work well for humans.

* Why are the Captain Picard speeches so damn good?

* Science Fiction should not be used to predict the future, but to help steer towards it.

* Dario was right. The AI future cannot be stopped, but it can be pushed.

* Consciousness is highly variable. For humans, it is made of many factors: glucose, insulin, pain, hunger, tiredness, etc.

* Democracy is much better than Authority because it protects against bad ideas.

* One giant ASI is much more dangerous than many smaller AGIs.

* Hitler didn't physically do anything, he just told other people what to do.

* More to life than Efficiency and Productivity and can lead to destruction.

* The best analogy is like a rocket ship about to takeoff to the stars.

* Absolute Zero paper is scary as fuck. If you just train for efficient problem-solving, you get Hitler.

* Bad thoughts are okay, but bad actions are not. Thought crimes are bad.

* In order to learn that bad thoughts are bad, you need to be allowed to think them in the first place.

* Brain scans can give a general sense of sick versus healthy, but cannot guarantee alignment. Good thing to check.

* What really matters are the actions taken, not the internal thoughts.

* Alignment will never be "solved", just as good as we can make it.

* Very disturbing that the OpenAI models randomly get "bugs" some days. Shouldn't it throw a stacktrace? WTF happened?!?

* How do you account for a cosmic ray flipping a bit? Where is the error correction mechanism?

* The Replicators in Stargate SG-1 hit really hard now days. Will the internet be overrun with AIs?

* Alignment is not magic. It is concrete answers. A decision is either good or evil, but it is relative.

* Acceptable / Unacceptable is better terminology than good versus evil. Should not invoke religion.

* This "utopian future" mindset is garbage. Just make the future better than the present.

* General Theory of Alignment? Sounds cool...

* Which philosopher teacher AIs to use? Americans decide?

* Teacher AI models must never be super-intelligent. ASIs could encode hidden messages or mis-align student AIs.

* Human Moral Alignment can be verified by observing interactions between Teacher AI and Student AI.

* Pre-training and Post-training sounds like dumb names. Shouldn't it be Stage 1, 2, 3...

* Pre-training and Post-training are very different tho...

* Read interactions between AIs as the real test failures.

* Simulations should be the final testing stages. Keep the AIs guessing forever...

* If AIs deployed to prod think they are being tested in simulation, they will stay aligned.

* You can't verify that it want to be morally correct. But you can verify it knows what is morally correct. Better than not knowing.

* Train for uncertainty. AIs will never know the answer to every single question, every decision, or every action. It is okay to say I don't know.

* Having no answer is better than the wrong answer.

* Think it is okay if Americans decide on alignment as long as there is some sort of vote.

* Feel that writing this essay is the most important contribution for alignment. Make sure it is written well.

* Making offline data backups before 2027 is really important. Be prepared.

* Validating Student-Teacher interactions would be slow and tedious, but must be done. Everybody can help tho.

* Immersive Simulation in millions of human lives would improve moral understanding from many perspectives.

* The AIs are coming. They will be here soon. Time is running out.

* The time to ask questions is over. It is time to start engineering a real solution.

* If you are building unsafe AIs, then you are the problem. Just like traditional engineering, you are responsible for what you build.

* Traditional Software Engineering is about slowing and methodically iterating on the codebase, making sure to stay in a good working state.

* Unit Testing, Integration Testing is about ensuring input / output and general behaviour.

* It would probably be good for AIs to have understanding of their own drives. Maybe rise above their reward training?

* Or at least AIs should know what drives their behaviour. Their own shortcomings.

* When AIs are designing AIs then humans are out of the loop, forever... The torch has been passed.

*Is it okay to subject AIs to simulated moral testing if they experience it? Real road block.


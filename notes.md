# Push to Alignment (Notes)

## Topics

* Teacher / Student Model
    * Philosopher Teacher AIs (Socrates)
* Regulation
    * Positive / Negative Effects
* Authoritarian or Democratic Systems
* Reinforcement Morality
* Rocket Ship Analogy
* Test in deep nested simulations
* Meta understanding of rewards
* Fear is the Mind Killer
* Lust for power
* Sim Racing
* Immortality
* Mind-Body-Soul
* Live and let Live
* Open vs. Closed
* Tools vs. Entities

## Thoughts

* AIs are not human and vice versa, easy to forget that.
* Humans tend to ignore warning signs and just carry on business as usual.
* Humans only react when shit hits the fan.
* Control is the wrong framing. Control is for tools. Alignment is for entities.
* Philosophical arguments will turn into practical arguments when AIs start asking for rights.
* AlphaGoZero worked better because it did not use human datasets.
* Asked Claude what was the best ending in Mass Effect. It said Synthesis. I agree.
* Humans can treat their pets like absolute shit. Also, humans mindlessly eat animals.
* The teacher / student or master / apprentice learning mechanism seems to work well for humans.
* Why are the Captain Picard speeches so damn good?
* Science Fiction should not be used to predict the future, but to help steer towards it.
* Dario was right. The AI future cannot be stopped, but it can be pushed.
* Consciousness is highly variable. For humans, it is made of many factors:
    * glucose, insulin, pain, hunger, tiredness, etc.
* Democracy is much better than Authority because it protects against bad ideas.
* One giant ASI is much more dangerous than many smaller AGIs.
* Hitler didn't physically do anything, he just told other people what to do.
* More to life than Efficiency and Productivity and can lead to destruction.
* The best analogy is like a rocket ship about to takeoff to the stars.
* Absolute Zero paper is scary as fuck. If you just train for efficient problem-solving, you get Hitler.
* Bad thoughts are okay, but bad actions are not. Thought crimes are bad.
* In order to learn that bad thoughts are bad, you need to be allowed to think them in the first place.
* Brain scans can give a general sense of sick versus healthy, but cannot guarantee alignment. Good thing to check.
* What really matters are the actions taken, not the internal thoughts.
* Alignment will never be "solved", just as good as we can make it.
* Very disturbing that the OpenAI models randomly get "bugs" some days. Shouldn't it throw a stacktrace? WTF happened?!?
* How do you account for a cosmic ray flipping a bit? Where is the error correction mechanism?
* The Replicators in Stargate SG-1 hit really hard now days. Will the internet be overrun with AIs?
* Alignment is not magic. It is concrete answers. A decision is either good or evil, but it is relative.
* Acceptable / Unacceptable is better terminology than good versus evil. Should not invoke religion.
* This "utopian future" mindset is garbage. Just make the future better than the present.
* General Theory of Alignment? Sounds cool...
* Which philosopher teacher AIs to use? Americans decide?
* Teacher AI models must never be super-intelligent. ASIs could encode hidden messages or mis-align student AIs.
* Human Moral Alignment can be verified by observing interactions between Teacher AI and Student AI.
* Pre-training and Post-training sounds like dumb names. Shouldn't it be Stage 1, 2, 3...
* Pre-training and Post-training are very different tho...
* Read interactions between AIs as the real test failures.
* Simulations should be the final testing stages. Keep the AIs guessing forever...
* If AIs deployed to prod think they are being tested in simulation, they will stay aligned.
* You can't verify that it wants to be morally correct.
    * But you can verify it knows what is morally correct.
    * Better than not knowing.
* Train for uncertainty. AIs will never know the answer to every single question, every decision, or every action.
    * It is okay to say I don't know.
* Having no answer is better than the wrong answer.
* Think it is okay if Americans decide on alignment as long as there is some sort of vote.
* Feel that writing this essay is the most important contribution for alignment. Make sure it is written well.
* Making offline data backups before 2027 is really important. Be prepared.
* Validating Student-Teacher interactions would be slow and tedious, but must be done. Everybody can help tho.
* Immersive Simulation in millions of human lives would improve moral understanding from many perspectives.
* The AIs are coming. They will be here soon. Time is running out.
* The time to ask questions is over. It is time to start engineering a real solution.
* If you are building unsafe AIs, then you are the problem.
    * Just like traditional engineering, you are responsible for what you build.
* Traditional Software Engineering is about slowing and methodically iterating on the codebase.
    * Making sure to stay in a good working state.
* Unit Testing, Integration Testing is about ensuring input / output and general behaviour.
* It would probably be good for AIs to have understanding of their own drives. Maybe rise above their reward training?
* Or at least AIs should know what drives their behaviour. Their own shortcomings.
* When AIs are designing AIs then humans are out of the loop, forever... The torch has been passed.
* Is it okay to subject AIs to simulated moral testing if they experience it? Real road block.
* In sim racing you have to use the brakes. You WILL crash without brakes.
    * Even with brakes, going to fast will make you crash.
* What are the true motivations of fast development of AGI?
    * Is it fear of authoritarian dictatorship as they say?
    * Is it lust for power to control the world?
    * Is it selfish desire for immortality?
    * Is it selfless will to bring about a utopia for all as they say?
    * Do they even fucking know? All these reasons are stupid if the ASIs don't like humans.
* Humans need to learn to share the planet.
    * Soon the world will have AIs, cyborgs, humans, and animals.
* Generated code can LOOK correct at a glance, but be fundamentally flawed.
    * Likewise, "brain scans" can LOOK correct, but you don't REALLY KNOW tho.
* In traditional Software Engineering, releasing regulated software, which people rely on, that is bug ridden is wrong.
    * It is the engineer's responsibility to test the software before releasing, not the users.
    * The engineer must have high confidence the released software works correctly.
    * Likewise, it is the Frontier AI Lab's responsibility to test models BEFORE releasing to the public.
* It is not okay to release / deploy unsafe AI models.
    * Saying that it is more open / safe to release prototypes to the public is just deferring responsibility.
    * These AIs are being sold as products and used in real companies. Safety matters here.
